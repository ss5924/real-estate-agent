import json
from openai import OpenAI
from src.personal_memory import MemoryManager

from src.tools import (
    classify_query_for_tools,
    plan_from_user_query,
    get_news,
    search_vector_store,
    get_current_datetime,
    search_korean_law,
    llm_as_a_judge,
    check_policy_and_safety,
    get_user_summary,
)
from src.agent_constants import TOOLS
from src.agent_utils import init_session, call_llm, update_status
from src.prompts import MEMORY_PROMPT_TEMPLATE


def get_response(
    user_id: str,
    client: OpenAI,
    query: str,
    directive: str | None,
    continuous: bool = True,
    index=None,
    chunks=None,
    metadatas=None,
    session: list | None = None,
    status_callback=None,
):
    if session is None:
        session = []

    previous_session_size = len(session)

    # ì„¸ì…˜ ì´ˆê¸°í™” ë° ì¤€ë¹„
    session = init_session(session, directive, continuous)

    # ì§ˆì˜ ë³µì¡ë„ ë¶„ë¥˜
    classify_result = classify_query_for_tools(query, client)
    need_tools = classify_result.get("need_tools", False)

    final_answer = ""
    tool_results = {}

    # ê°„ë‹¨í•œ ì§ˆì˜ (Tools ë¶ˆí•„ìš”)
    if not need_tools:
        final_answer, tool_results, session = _handle_simple_query(
            client, session, query, directive, classify_result
        )

    # ë³µì¡í•œ ì§ˆì˜ (Tools + Planner + Judge)
    else:
        assert session is not None
        session.append({"role": "user", "content": query})

        # í”Œë˜ë„ˆ ë‹¨ê³„
        plan, tool_plan = _run_planner_phase(
            client, query, session, status_callback=status_callback
        )

        tool_results = {
            "_planner": plan,
            "_classifier": classify_result,
        }

        # íˆ´ ì‹¤í–‰ ë£¨í”„
        draft_answer = _run_tool_loop(
            user_id=user_id,
            client=client,
            session=session,
            tool_plan=tool_plan,
            tool_results=tool_results,
            status_callback=status_callback,
            index=index,
            chunks=chunks,
            metadatas=metadatas,
        )

        # Judge ë£¨í”„
        final_answer, judge_logs = _run_judge_loop(
            client=client,
            query=query,
            directive=directive,
            session=session,
            first_output=draft_answer,
            tool_results=tool_results,
            status_callback=status_callback,
        )
        tool_results.update(judge_logs)

        update_status(status_callback, "âœ… ë‹µë³€ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì¥ê¸° ë©”ëª¨ë¦¬ ì§€ëŠ¥í˜• ì—…ë°ì´íŠ¸
    if user_id:
        try:
            mm = MemoryManager()
            _update_memory_if_necessary(client, session, user_id, mm)
        except Exception as e:
            # ë©”ëª¨ë¦¬ ì €ì¥ì´ ë©”ì¸ ë¡œì§ì„ ë°©í•´í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  íŒ¨ìŠ¤
            print(f"Memory update failed: {e}")

    return final_answer, tool_results, session, previous_session_size


def _update_memory_if_necessary(
    client: OpenAI, session: list, user_id: str, mm: MemoryManager
):
    recent_messages = []
    # ë’¤ì—ì„œë¶€í„° 10ê°œ ì •ë„ë§Œ ë³´ë˜, userë‚˜ assistantì˜ 'ëŒ€í™” ë‚´ìš©'ë§Œ ì¶”ë ¤ëƒ…ë‹ˆë‹¤.
    for msg in reversed(session):
        if len(recent_messages) >= 6:  # ìµœëŒ€ 6í„´ë§Œ í™•ì¸
            break

        # tool ë©”ì‹œì§€ë‚˜ tool_callsëŠ” ë©”ëª¨ë¦¬ ìš”ì•½ì— êµ³ì´ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œì™¸ (API ì—ëŸ¬ ë°©ì§€)
        if msg["role"] in ["user", "assistant"] and msg.get("content"):
            # ìˆœì„œë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì•ì— ì‚½ì… (reversedë¡œ ëŒê³  ìˆìœ¼ë¯€ë¡œ)
            recent_messages.insert(0, {"role": msg["role"], "content": msg["content"]})

    # ë‚´ìš©ì´ ë„ˆë¬´ ì—†ìœ¼ë©´ ì¤‘ë‹¨
    if not recent_messages:
        return

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_msg = {
        "role": "system",
        "content": MEMORY_PROMPT_TEMPLATE,
    }

    # LLM í˜¸ì¶œ
    messages = []
    messages.append(system_msg)
    messages.extend(recent_messages)

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            response_format={"type": "json_object"},
        )
        content = response.choices[0].message.content

        if not content:
            raise ValueError("Empty response from LLM")

        result = json.loads(content)

        should_update = result.get("update_needed", False)
        new_memory = result.get("memory_content", "").strip()

        if should_update and new_memory:
            # ê¸°ì¡´ ë©”ëª¨ë¦¬ ë¡œë“œ
            existing = mm.get_user_summary(user_id) or ""

            if existing:
                combined_memory = f"{existing}\n- {new_memory}"
            else:
                combined_memory = f"- {new_memory}"

            # ì €ì¥
            mm.save_user_summary(user_id, combined_memory)
            print(f"ğŸ“ [Memory Updated] {new_memory}")
        else:
            # ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
            pass

    except Exception as e:
        print(f"Error during memory judgement: {e}")


def _handle_simple_query(
    client: OpenAI,
    session,
    query: str,
    directive: str | None,
    classify_result: dict,
):
    session.append({"role": "user", "content": query})
    msg = call_llm(client, session)
    final_answer = msg.content
    session.append({"role": "assistant", "content": final_answer})

    tool_results = {
        "_mode": "simple_answer",
        "_classifier": classify_result,
    }
    return final_answer, tool_results, session


def _run_planner_phase(client: OpenAI, query: str, session, status_callback=None):
    update_status(status_callback, "âœï¸ í”Œë˜ë„ˆê°€ ì§ˆë¬¸ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    plan = plan_from_user_query(query, client)
    refined_q = plan.get("refine_question", query)
    intent = plan.get("intention", "")
    tool_plan = plan.get("tool_plan", [])

    user_content = f"[ì‚¬ìš©ì ì›ë¬¸]\n{query}\n\n[ì •ì œëœ ì§ˆë¬¸]\n{refined_q}"
    if intent:
        user_content += f"\n\n[ì˜ë„ ì¶”ë¡ ]\n{intent}"

    session.append({"role": "system", "content": user_content})
    session.append(
        {
            "role": "system",
            "content": (
                "ë‹¤ìŒì€ ìƒìœ„ í”Œë˜ë„ˆê°€ ì¶”ì²œí•œ íˆ´ ì‚¬ìš© ê³„íšì…ë‹ˆë‹¤. "
                "í•„ìš”ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ì°¸ê³ í•˜ì„¸ìš”.\n"
                + json.dumps(plan, ensure_ascii=False)
            ),
        }
    )
    return plan, tool_plan


def _execute_tool_call(
    user_id: str,
    func_name: str,
    args: dict,
    client: OpenAI,
    index=None,
    chunks=None,
    metadatas=None,
):
    if func_name == "get_news":
        return get_news(args["topic"])
    if func_name == "search_vector_store" and index is not None:
        return search_vector_store(
            client,
            args["query"],
            index,
            chunks,
            metadatas,
            top_k=args.get("top_k", 3),
        )
    if func_name == "get_current_datetime":
        return get_current_datetime()
    if func_name == "search_korean_law":
        return search_korean_law(**args)
    if func_name == "check_policy_and_safety":
        return check_policy_and_safety(args["user_query"], args["answer"], client)
    if func_name == "get_user_summary":
        return get_user_summary(user_id=user_id)
    return {"error": f"ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜: {func_name}"}


def _run_tool_loop(
    user_id: str,
    client: OpenAI,
    session,
    tool_plan,
    tool_results: dict,
    status_callback=None,
    index=None,
    chunks=None,
    metadatas=None,
):
    planned_steps = len(tool_plan or [])
    base_loops = 1 if planned_steps == 0 else planned_steps
    MAX_TOOL_LOOPS = min(base_loops + 2, 6)

    if planned_steps == 0:
        update_status(
            status_callback, "ğŸ§­ í”Œëœ ë¶„ì„ ê²°ê³¼, í•„ìš”í•œ ê²½ìš°ì—ë§Œ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤..."
        )
    else:
        update_status(
            status_callback,
            f"ğŸ§­ í”Œëœ ë¶„ì„ ê²°ê³¼, ìš°ì„  {planned_steps}ê°œì˜ ë„êµ¬ ì‚¬ìš©ì´ ì¶”ì²œë˜ì—ˆìŠµë‹ˆë‹¤.",
        )

    draft_answer = None
    loop_idx = 0

    while True:
        if loop_idx >= MAX_TOOL_LOOPS:
            break

        update_status(
            status_callback,
            f"ğŸ” ì™¸ë¶€ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ ìë£Œë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤... ({loop_idx+1}/{MAX_TOOL_LOOPS})",
        )

        msg = call_llm(
            client,
            session,
            tools=TOOLS,
            tool_choice="auto",
        )

        # íˆ´ í˜¸ì¶œ ì—†ì´ ë°”ë¡œ ë‹µë³€ì´ ì˜¤ë©´ ë£¨í”„ ì¢…ë£Œ
        if not getattr(msg, "tool_calls", None):
            draft_answer = msg.content
            session.append({"role": "assistant", "content": draft_answer})
            break

        # íˆ´ í˜¸ì¶œ ì²˜ë¦¬
        session.append({"role": "assistant", "tool_calls": msg.tool_calls})

        for t in msg.tool_calls:
            func_name = t.function.name
            args = json.loads(t.function.arguments)

            try:
                result = _execute_tool_call(
                    user_id,
                    func_name,
                    args,
                    client,
                    index=index,
                    chunks=chunks,
                    metadatas=metadatas,
                )
            except Exception as e:
                result = {"error": str(e)}

            tool_results[func_name] = result

            if func_name == "get_user_summary":
                # ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•´ get_user_summary ê²°ê³¼ëŠ” ì„¸ì…˜ì— ì €ì¥í•˜ì§€ ì•ŠìŒ
                continue

            session.append(
                {
                    "role": "tool",
                    "tool_call_id": t.id,
                    "name": func_name,
                    "content": json.dumps(result, ensure_ascii=False),
                }
            )

        loop_idx += 1

    if draft_answer is None:
        update_status(
            status_callback, "ğŸ§© ìˆ˜ì§‘í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
        )
        msg = call_llm(client, session)
        draft_answer = msg.content
        session.append({"role": "assistant", "content": draft_answer})

    return draft_answer


def _run_judge_loop(
    client: OpenAI,
    query: str,
    directive: str | None,
    session,
    first_output: str,
    tool_results: dict,
    status_callback=None,
):
    update_status(status_callback, "ğŸ§ª LLM Judgeê°€ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤...")

    current_attempt = 1
    max_retries = 3
    output = first_output
    last_judgement = None
    judge_logs = {}

    while current_attempt <= max_retries:
        judge_input_content = json.dumps(
            {
                "user_query": query,
                "system_directive": directive,
                "tool_call_results": tool_results,
                "first_response": output,
            },
            ensure_ascii=False,
        )
        try:
            judgement_str = llm_as_a_judge(judge_input_content, client)
            if not judgement_str:
                raise ValueError("Empty response from LLM")

            judgement = json.loads(judgement_str)

            key = f"llm_as_a_judge_attempt_{current_attempt}"
            judge_logs[key] = judgement
            last_judgement = judgement

            score = judgement.get("score")
            # 4.0 ë¯¸ë§Œ + ì¬ì‹œë„ ê°€ëŠ¥í•˜ë©´ ì¬ìƒì„±
            if (
                isinstance(score, (int, float))
                and score < 4.0
                and current_attempt < max_retries
            ):
                reason = judgement.get("reason", "ì‚¬ìœ  ì—†ìŒ")

                update_status(
                    status_callback,
                    f"ğŸ” Judge ì ìˆ˜ {score}ì  â†’ ë‹µë³€ì„ ë‹¤ì‹œ ë‹¤ë“¬ëŠ” ì¤‘ì…ë‹ˆë‹¤... ({current_attempt}/{max_retries})",
                )

                retry_prompt = (
                    f"ë‹¹ì‹ ì˜ ì´ì „ ì‘ë‹µì´ í’ˆì§ˆ ì ìˆ˜ {score}ì ì„ ë°›ì•˜ìœ¼ë©°, ì‚¬ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: '{reason}'\n"
                    f"ì´ ì‚¬ìœ ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ë” ì •í™•í•˜ê³ , ì‹œìŠ¤í…œ ì§€ì‹œë¬¸ì„ ë” ì˜ ì¤€ìˆ˜í•˜ë©°, "
                    f"ì¶œë ¥ í˜•ì‹([í•µì‹¬ ìš”ì•½] / [ìƒì„¸ ì„¤ëª…] / [ì¶œì²˜])ì„ ìœ ì§€í•˜ë„ë¡ ë‹¤ì‹œ ë‹µë³€í•´ì£¼ì„¸ìš”. "
                    f"í•„ìš”ì‹œ ì´ì „ì— ì°¸ì¡°í•œ ë„êµ¬ í˜¸ì¶œ ê²°ê³¼(tool_call_results)ë¥¼ ë‹¤ì‹œ í™œìš©í•´ë„ ë©ë‹ˆë‹¤."
                )

                session.append({"role": "system", "content": retry_prompt})
                retry_msg = call_llm(client, session)
                output = retry_msg.content
                session.append({"role": "system", "content": output})

                current_attempt += 1
            else:
                break

        except Exception as e:
            key = f"llm_as_a_judge_attempt_{current_attempt}"
            judge_logs[key] = {
                "error": f"Judge í˜¸ì¶œ ë˜ëŠ” íŒŒì‹± ì˜¤ë¥˜: {e}",
                "original_output": output,
            }
            break

    if last_judgement is not None:
        judge_logs["_judge_last"] = last_judgement

    return output, judge_logs
